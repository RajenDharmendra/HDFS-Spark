CPU = central processing unit
"brain"
every process is handled by the CPU
typical CPU can execute over 2.5 billion operations per second

CPU can store small amounts of data inside itself = registers
holds the data the CPU is currently working with

ex: script reads in 40 MB data file + analyzes
when you execute the script, the instructions are loaded into the CPU
CPU then instructs the computer to take the 40 MB from disk + store data in RAM
if you want to sum a column of data, the CPU will take 2 numbers at a time and sum them together
the accumulation of the sum needs to be stored somewhere while the CPU grabs the next number
the cumulative sum will be stored in the register
registers make computations more efficient 
registers avoid having to send data unnecessarily back + forth between RAM and CPU

2.5 Gigahertz CPU processes 2.5 billion operations per second

memory (RAM)
"scratch paper"
data gets temporarily stored before getting sent to CPU
ephemeral storage

it takes 250x longer to find and load a random byte from memory vs process that byte in CPU
ex: in the time it takes to load an hours worth of random tweets in memory, a CPU could process over a week of tweets

characterists of memory
- efficiently loads data for CPU use
- ephemeral 
- expensive 

rather than rely on an expensive super computer with lots of memory like everyone else in the late 90s
Google took a different route
leveraged long-term storage on cheap pre-used hardware
distributed clusters = now industry standard

storage (SSD)
"filing cabinet"
long-term data storage
when a program runs, the CPU will direct the memory to temporarily load data from long term storage

network
access to the outside world
moving data across the network is the most common bottleneck when working with big data

ex: processing an hour of tweets (4.3 GB)
if data is in memory = takes 30ms to process
if data is in SSD and needs to be loaded into memory first = takes 0.5s to process
if data needs to be pulled down from twitter API via internet = 30 seconds

usually takes 20x longer to process data when you need to pull it down from another machine first
distributed systems try to minimize shuffling data between nodes as much as possible

key ratios
fastest
CPU = 200x faster vs loading data randomly from memory
memory = 15x faster vs long term storage SDD
SSD = 20x faster vs network

##################
what is big data?
##################
starting definition = cannot process the data on a single machine
need a cluster of computers to process the data

my machine has 32GB of RAM vs 1TB of SSD

what happens when you try to run a script on a local machine using a data file that is larger than what your RAM can hold?
the script will not work because
- memory could not load data quickly from storage
- CPU could not load data quickly from memory

CPU directed SSD to load 8GB of data into RAM, leaving no other memory for other processes + OS
CPU processed 8GB quickly + passed the 10GB results back to RAM > written to output file on SSD
the next 8GB then loaded from SSD > RAM = 3000x slower to load data from SSD vs actual CPU processing
the computer is spending most of its time moving data in and out of memory
most CPU activity is from coordinating data IO > thrashing when system is overwhelmed by IO

##############
hadoop ecosystem
##############
distributed computing
each node has its own CPU + memory 
communicate with each other by passing messages

parallel computing
form of distributed computing
the distinction is multiple CPUs have access to shared memory

HDFS = data storage
splits data into 64 or 128 MB chunks 
stores chunks across a cluster of computers

Hadoop MapReduce = programming model
MAP
each map step reads a partition from HDFS
converts input into key, value pair (aka tuple)

SHUFFLE
shuffle the kv pairs across the cluster so all keys are on the same node

REDUCE
compute the final result
values for a given key are combined

YARN = resource manager 
schedules jobs across the cluster
keeps track of compute resources available 
then assigns those available resources to specific tasks
task = smallest unit of operation

###########
spark
###########
each node is responsible for a set of operations
at the end we combine the partial results to get the final answer 
master node = responsible for orchestrating tasks
worker nodes = perform actual operations

local mode
everything happens on a single machine
useful for prototyping

spark shell 
directly interact with driver program

PySpark API allows you to write Python programs in Spark
ensures that your code uses functional programming practices
underneath the hood, the Python code uses py4j to make calls to the Java Virtual Machine (JVM)

https://en.wikipedia.org/wiki/Java_virtual_machine
JVM allows you to run Java programs
also run programs written in other languages (ex: scala) that are compiled to Java bytecode
interacting with JVM allows programmers to write Java and not worry about the underlying hardware the program is running on

https://www.py4j.org/
Py4J enables Python programs running in the Python interpreter to dynamically access Java objects in the Java virtual machine (JVM)
methods are called as if the objects reside in the Python interpreter 
Java collections can be accessed via standard Python collection methods

Spark context = entry point to Spark API
connects the cluster with the application
access Spark context through SparkSession
SparkSession = used to perform computations across the cluster

Spark context method parallelize 
takes Python object + distributes object across the cluster

#######################
functional programming
#######################
in math
a function can only give you only give you one answer when you give it an input
f(x) = x + 5
f(3) = 3 + 5 = 8

vs Python
running the same code over and over again with the same input will output different results
w = -1
def f(x):
	global w
	w += 1
	return x + w + 5

when you have dozens of machines running code in parallel
and sometimes you need to restart a calculation if there was a failure 
these unintended side effects can lead to trouble

your functions should not have side effects on variables outside their scope
this could interfere with other functions running on your cluster

whenever your functions run on some input data it cannot alter the input data in the process

Spark DAG
every Spark function makes a copy of its input data so it never changes the original parent data
immutable = Spark does not change/mutate the input data 

chain together multiple functions so each function takes care a small chunk of the work

lazy evaluation
before Spark does any work on the data, it builds step-by-step instructions

ex: bread baking analogy
look at the receipe before you start mixing the ingredients together  

see what you need and grab all ingredients from the pantry at once
vs going back and forth to the pantry to get what you need = equivalent of thrashing
thrashing = compute gets overwhelmed from the process of transfering data from SSD to RAM

generally combine all wet ingredients together first, then all dry ingredients
these multi-step combos = stages in Spark
in each stage, the worker node divides up the input data + runs the task for that stage

########################
Spark SQL approaches
########################
comes from math concept of mapping inputs to outputs
apply a function to each value in the input
common to use lambda functions

df.map(lambda song : song.lower())

why is it called a lambda function?
https://palmstroem.blogspot.com/2012/05/lambda-calculus-for-absolute-dummies.html

imperitive = how
provide all the details on how to accomplish a task
provide specific tranformations to apply to the data frame

declarative = what = Spark SQL
ex: spark.sql("SELECT * FROM TABLE");
more concerned with what we want the result to be
there is a layer of abstraction for the user 

Why might you prefer to use SQL over data frames? Why might you prefer data frames over SQL?
both Spark SQL and Spark Data Frames are part of the Spark SQL library
both use the Spark SQL Catalyst Optimizer to optimize queries

You might prefer SQL over data frames because the syntax is clearer especially for teams already experienced in SQL

Spark data frames give you more control
break down your queries into smaller steps, which can make debugging easier
can also cache intermediate results or repartition intermediate results

#####################
RDDs vs DataFrames and Datasets
https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html
#####################
RDD
immutable distributed collection of data
partitioned across nodes in the cluster
can be operated on in parallel 

When to use RDDs?
you want low level control of transformations + actions performed on the data set
data is unstructured (ex: streams of text)
you don't care about imposing a schema 

DataFrames
immutable distributed collection of data
untyped JVM objects
data is organized into named columns, like a table in a relational database 

DataSet
collection of strongly typed JVM objects
dictated by the class you define in Scala/Java
not available in Python

##################
spark macos install
##################
used homebrew
https://www.freecodecamp.org/news/installing-scala-and-apache-spark-on-mac-os-837ae57d283f/
https://medium.com/@purmac/using-standalone-spark-with-amazon-s3-1ff72f2cf843 
https://vinta.ws/code/setup-spark-on-macos.html

update config file 
/usr/local/Cellar/apache-spark/2.4.3/libexec/conf/spark-defaults.conf

use s3n:// protocol 
https://stackoverflow.com/questions/33356041/technically-what-is-the-difference-between-s3n-s3a-and-s3

###########
logging
###########
logs are spread out amongst the different nodes
Spark UI allows for an easy access of the logs

can specify default log level in conf file
or 
can specify in SparkContext
spark.sparkContext.setLogLevel("ERROR")

####################
questions to ask
####################
average # of minutes the user listening to the app per day
# of songs per day (including repeat and partial listens)

monthly active users (# of unique users who have listened to at least 1 song per month)
daily active users (total # and % of monthly active users)

total # of paid + unpaid users
# of ads delivered per month

cohort analysis
experienced users interact in the app differently vs new users
% of users who upgrade to paid accounts within first 3 months
% of users who cancel in the first 3 months

####################
optimization intro
####################
data skew
Pareto principle 
80% of data comes from 20% of your users

run a quick Spark job to get a summary of your data
sample 5% of your data to get a sense of data skew

if certain partitions process significantly more data than others 
- add intermediate data processing step with an alternative key
- adjust the spark.sql.shuffle.partitions if necessary

options
- change workload division
- break the data into smaller parts = partitions

insufficient resources
when you have a slow job it's useful to understand
- how much data you are actually processing (compressed data can make this tricky)
- if you can decrease the amount of data processed by filtering or aggregating to lower cardinality 
- is resource utilization reasonable?

different stages of a spark job will vary greatly in their resource needs
loading data = I/O heavy
some stages may require a lot of memory
others may need a lot of CPU
use Spark UI and logs to determine these metrics

OOM > increase # of partitions 

as much as possible try to avoid shuffling data 
in practice this means you need to perform joins and grouped aggregations as late as possible

#######################################################################################################3
Apache Spark philosophy
	unified
		wide variety of operations
			simple as loading data and querying using SQL
			complicated as using machine learning
		for example
			you can load data in using SQL
			then evaluate a machine learning model over the data
			spark can combine these steps into one scan over the data
	computing engine
		only responsible for performing computations
		not for storage
		works with a variety of data sources
	libraries
		provide a unified API for common data analysis tasks
		2 sets of APIs
			low level "unstructured" APIs
			high level Structured APIs

Basic Architecture
	manage and coordinate execution of tasks on data across a cluster of computers
	we submit Spark apps to cluster managers > grant resources to app to do work
	cluster manager
		controls physical machines
		allocates resources to Spark app
	use spark shell for exploratory work
	use spark-submit to send app code to cluster for stand-alone execution

Spark App
	driver process
		runs main() function
		sits on a node in the cluster
		responsible for
			maintaining info about Spark app
			responding to user/program input
			analyzing distributing and scheduling work across executors
		driver can be "driven" from a number of different languages via Spark's language APIs
		control Spark app through driver process
	executor processes
		executes code assigned to it by the driver
		reporting state of computation back to the driver node
		almost always running Spark scala code

SparkSession
	driver process manifests itself to the user as a SparkSession object
	entrance point to running Spark code
		1-to-1 correspondence b/w SparkSession and Spark app
	interactive Spark session
		./bin/spark-shell
		./bin/pyspark
		implicitly create SparkSession in interactive mode
	submit precompiled application to Spark engine
		spark-submit

DataFrame
	represents a table of data with rows and columns
	distributed collection = exists on multiple machines
	best to use DataFrame over Datasets, SQL Tables, and RDDs
	available in all languages

Partitions
	Spark breaks data up into chunks = partitions
	partition = collection of rows that sit on one physical machine in our cluster
	parallelism of 1
		only 1 partition
			even if you have thousands of executors available
			data is only available in 1 chunk
		only 1 executor
			even if you have multiple partitions
			only 1 computational resource
	with DF
		do not manipulate partitions manually
		specify high level transformations of data > Spark determines how to split data

Transformations
	core data structures are immutable
	specify how you want a DF to change to get the resulting DF
	lazy evaluation
	2 types of transformations
		narrow transformations = narrow dependency
			1 input partition > 1 output partitionn
			pipelining
				specify multiple filters on DF for a transformation
				all filters performed in memory
		wide transformation = wide dependency
			1 input partition > multiple output partitions
			computation requires a shuffle
				exchange partitions across the cluster
				Spark needs to write results to disk 

Lazy Evalution
	Spark will wait until the very last moment to execute the graph of computation instructions
	predicate pushdown
		Spark can optimize the entire data flow from end to end
		for example
			build a large spark job and then
			the last filter only requires to fetch 1 row from source data >
			only access the single record we need

Action
	trigger the computation
	instructs Spark to compute a result from a series of transformations
	3 types
		view data in the console
		collect data to native objects
		write to output data sources

End-to-End Example
	use DataFrame reader assoc with SparkSession to read in data
		schema inference = ask Spark to figure out schema
		header = true specifies first row is a header not data
	each resulting DataFrame has a set of columns with unspecified # of rows
		reading data = transformation = lazy evaluation
		Spark only took a peek at a couple rows to guess schema
	explain()
		usedful for debugging
		top - end result
		bottom - data sources
	shuffle partitions
		by default Spark will output 200 shuffle partitions
		spark.conf.set("spark.sql.shuffle.partitions", "5")
		will be used when performing a wide transformation
	DF methods
		many methods accept a string for the column name
		groupBy > RelationalGroupedDataset
			fancy DataFrame
			specify key we are going to group by
			the perform aggregation on that grouping
			grouping is specified but user needs to specify aggregation before it can be queried further

Structured APIs
	3 core distributed collection APIs
		Datasets
			Used for statically typed code in Java and Scala
			not available for Python and R bc dynamically typed code 
		Dataframes
		SQL tables and views
			no performance difference between writing SQL queries or writing DataFrame code
				allows you to query data using SQL
				SQL query on DataFrame returns another DataFrame
			createOrReplaceTempView
				make DataFrame into a table or view
	Execution
		1. Write DataFrame/SQL code
		2. If valid code, Spark converts to Logic Plan
			Resolved logical plan > optimized logical plan
		3. Spark transforms Logic Plan to Physical Plan checking for optimizations along the way
		4. Spark then executes the Physical Plan (RDD manipulations) on the cluster

DataFrame
	distributed table like collection with well defined rows and columns
		each row must have same number of columns
			to Spark (in Scala) DataFrames are Datasets of Type Row
		column type information must be consistent
	schema
		define column name and type 
			schema-on-read is fine for ad-hoc analysis
			good idea to define schema for prod ETL jobs
		schema =StructType 
			made up of StructFields
				name
				type
				boolean
					nullable specifies whether the column can contain missing or null values
		Column Type can be
			simple type
				integer or string
			complex type
				array or map
			null value
		df.printSchema()
	partitioning
		defines the physical distribution across the cluster
		partitioning scheme defines how it is allocated
		can set based on values in a certain column or non-deterministically
	columns
		select, manipulate, and remove columns from DataFrames
			these operations represent expressions
			expression = set of tranformations on 1+ values in a record in a DF
		logical constructs that represent a value computed on a per record basis by means of an expression
		df.col("column_name")
			specify column
			useful when performing a join
		df.columns returns list of column names 
	rows
		can create rows manually but rows do not have schema
		if you append Row to DataFrame, must match DF schema
		can access specific field using index

RDD
	DF transformations are really RDD transformations
		Spark UI describes execution in terms of RDDs
	SparkContext
		entry point for lower level API functionality
		access SparkContext through SparkSession
			SparkSession = used to perform computation across a Spark cluster
	RDD = immutable partitioned collection of records that can be operated on in parallel
	unlike DF
		these records are just Java/Python objects
		do not contain known schema
	Python can lose substantial when using RDDs
		running UDFs row by row

Developing Python Spark Apps
	Package multiple python files into egg or zip 
		use --py-files arg with spark-submit 
		submit .py, .zip, or .egg files to be distributed with app

Launching apps with spark-submit
	--class 
		main class for Java/Scala apps
	--master
		local
			local[*] = run on all cores of your machine
		yarn
	--deploy-mode
		client (default) launch the driver program via CLI
		cluster = launch job as as step
	--jars
		comma sep list of jars to include on the driver and executor classpaths
	--py-files
		comma sep list of .py, .zip, or .egg files to place on PYTHONPATH for Python apps
	--files
		comma sep list of files to include in working dir of each executor
	--conf
		PROP=VALUE
	--properties
		path to a file from which to load extra properties
		defaults to conf/spark-defaults.conf
	--driver-memory
		default 1024M
		needs to be set via command line in client mode
		can be set in SparkConf in app if cluster mode
	--executor-memory
		default 1G

	YARN specific
	--num-executors
		number of executors to launch
		default = 2
		if dynamic allocation is enabled, the intial number of executors will be at least num specified

	EMR config
		do not use maximizeResourceAllocation=True
		recent EMR versions have dynamicAllocation enabled
			application may give resources back to the cluster if they are no longer used
			request them again later when there is demand
		using Spark on EMR will determine the ideal # of executors for you
		only need to specify executor memory
		maybe driver memory and overhead as well

Running on a cluster
	architecture review
		driver
			just a process on a physical machine
			responsible for maintaining the state of an app running on the cluster
			keeps track of tasks running on executors
			interfaces with cluster manager to get physical resources and launch executors
		executors
			processes that perform tasks assigned by driver
			report back state and results
		cluster manager
			responsible for maintaining a cluster of machines
			master and worker abstractions are tied to actual physical machines
				not processes like the driver and executors
	execution modes
		cluster mode
			user submits pre-compiled JAR, python script to cluster manager
			cluster manager launches driver on a worker node INSIDE the cluster
			in addition to executor processes
			cluster manager responsible for all processes
		client mode
			spark driver remains on client machine
			client machine responsible for managing driver process
			cluster manager responsible for executor processes
		local mode
			all on a single machine
			achieves parallelism through threads on a single machine
			for learning and testing


#########################################################
Spark Memory Mangement
https://0x0fff.com/spark-memory-management/
#########################################################

running same code on Spark 1.5.x and 1.6 will result in diff behavior
	starting Spark 1.6.0 memory management model changed
	can enable spark.memory.useLegacyMode if necessary

reserved memory
	RAM reserved for the system
		not used by spark in any way
		does not participate in Spark memory region size calculations
	hardcoded to 300MB
		can set spark.testing.reservedMemory but not recommended to use in prod
	sets the limit on what you can allocate for spark usage
		even if you wanted to give all the Java heap to spark to cache your data you can't
		need to set aside reserved memory for spark internal objects
	450MB min executor memory = 300MB reserved memory * 1.5 
		must give this minimum 150MB available to spark + user memory
		otherwise will get please use larger heap size error message

user memory
	store data structures
	totally up to you what would be stored in this RAM and how
	spark does not check if you respect this boundary or not > can lead to OOM error

spark memory
	memory pool managed by apache spark
	split into 2 regions
		storage memory
			store spark cached data
			store temp partition unroll data
				if there is not enough memory available to fit the whole partition > will write to disk
			store broadcast variables 
				in a cache with MEMORY_AND_DISK persistence
		execution memory
			store objects required during the execution of spark tasks
				ex: store shuffle intermediate buffer on map side in memory
				ex: store hash table for hash aggregation step
			also supports spilling to disk if not enough memory available
				but blocks cannot be forefully evicted by other threads (tasks)
		boundary set by spark.memory.storageFraction
			default to 0.5
			not static
			one region can grow by borrowing space from another
				cannot evict execution memory 
				can evict storage memory
					its just a cache of blocks stored in RAM
					can simply write to disk instead

java heap = total executor memory
450MB total executor memory minimum = 300MB reserved memory * 1.5
300MB default reserved memory used for spark internals
total executor memory - 300MB reserved memory = remaining user memory + spark memory
remaining = 25% user memory + 75% spark memory
user memory = (total executor memory - 300MB reserved memory) * (1.0 - 0.75 default spark.memory.fraction)
spark memory = (total executor memory - 300MB reserved memory) * 0.75
spark memory = default 50% storage memory + 50% execution memory
storage memory = (total executor memory - 300MB reserved memory) * 0.75 spark memory * 0.5 storage memory portion
